{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler, TensorDataset\n",
    "import math\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from torchvision.ops import DeformConv2d\n",
    "\n",
    "class DeformableTokenEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model, cycle_length):\n",
    "        \"\"\"\n",
    "        c_in: number of input channels\n",
    "        d_model: desired output embedding dimension\n",
    "        cycle_length: kernel size (and stride) in the temporal dimension,\n",
    "                      which corresponds to one cycle.\n",
    "        \"\"\"\n",
    "        super(DeformableTokenEmbedding, self).__init__()\n",
    "        # We'll treat the 1D sequence as a 2D input with height=1 and width=L.\n",
    "        # Define the offset convolution that outputs 2 * cycle_length channels (for x and y offsets).\n",
    "        self.offset_conv = nn.Conv2d(\n",
    "            in_channels=c_in,\n",
    "            out_channels=2 * cycle_length,\n",
    "            kernel_size=(1, cycle_length),\n",
    "            stride=(1, cycle_length)\n",
    "        )\n",
    "        # Define the deformable convolution:\n",
    "        # Input: [B, c_in, 1, L]\n",
    "        # Kernel size: (1, cycle_length), stride: (1, cycle_length)\n",
    "        self.deform_conv = DeformConv2d(\n",
    "            in_channels=c_in,\n",
    "            out_channels=d_model,\n",
    "            kernel_size=(1, cycle_length),\n",
    "            stride=(1, cycle_length),\n",
    "            padding=(0, 0)\n",
    "        )\n",
    "        # Initialize weights for both offset_conv and deform_conv.\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input x shape: [B, L, c_in]\n",
    "        # Permute to [B, c_in, L] and add a dummy height dimension -> [B, c_in, 1, L]\n",
    "        x = x.permute(0, 2, 1).unsqueeze(2)\n",
    "        # Compute the offsets using the offset_conv; output shape: [B, 2 * cycle_length, 1, L_out]\n",
    "        offsets = self.offset_conv(x)\n",
    "        # Apply deformable convolution. It uses the computed offsets.\n",
    "        out = self.deform_conv(x, offsets)\n",
    "        # out shape: [B, d_model, 1, L_out]\n",
    "        # Remove the height dimension and transpose to [B, L_out, d_model]\n",
    "        out = out.squeeze(2).transpose(1, 2)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import weight_norm\n",
    "import math\n",
    "\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=200):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model).float()\n",
    "        pe.require_grad = False\n",
    "\n",
    "        position = torch.arange(0, max_len).float().unsqueeze(1)\n",
    "        div_term = (torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)).exp()\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pe[:, :x.size(1)]\n",
    "\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model, cycle_length):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.tokenConv = nn.Conv1d(in_channels=c_in, out_channels=d_model,\n",
    "                                   kernel_size=cycle_length, stride = cycle_length)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tokenConv(x.permute(0, 2, 1)).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DataEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model, cycle_length, dropout=0.0):\n",
    "        super(DataEmbedding, self).__init__()\n",
    "\n",
    "        self.value_embedding = DeformableTokenEmbedding(c_in=c_in, d_model=d_model, cycle_length=cycle_length)\n",
    "        self.position_embedding = PositionalEmbedding(d_model=d_model)\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, d_model))\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.value_embedding(x)\n",
    "        x = torch.cat([self.cls_token.expand(x.size(0), -1, -1), x], dim=1)\n",
    "        x = x + self.position_embedding(x)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, attention, d_model, n_heads, d_keys=None,\n",
    "                 d_values=None):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "\n",
    "        d_keys = d_keys or (d_model // n_heads)\n",
    "        d_values = d_values or (d_model // n_heads)\n",
    "\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.inner_attention = attention\n",
    "\n",
    "        self.query_projection = nn.Linear(d_model,\n",
    "                                          d_keys * n_heads)\n",
    "        self.key_projection = nn.Linear(d_model,\n",
    "                                        d_keys * n_heads)\n",
    "        self.value_projection = nn.Linear(d_model,\n",
    "                                          d_values * n_heads)\n",
    "        self.sigma_projection = nn.Linear(d_model,\n",
    "                                          n_heads)\n",
    "        self.out_projection = nn.Linear(d_values * n_heads, d_model)\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "    def forward(self, queries, keys, values, attn_mask):\n",
    "        B, L, _ = queries.shape\n",
    "        _, S, _ = keys.shape\n",
    "        H = self.n_heads\n",
    "        x = queries\n",
    "        queries = self.query_projection(queries).view(B, L, H, -1)\n",
    "        keys = self.key_projection(keys).view(B, S, H, -1)\n",
    "        values = self.value_projection(values).view(B, S, H, -1)\n",
    "\n",
    "        sigma = self.sigma_projection(x).view(B, L, H)\n",
    "\n",
    "        out, series, prior, sigma = self.inner_attention(\n",
    "            queries,\n",
    "            keys,\n",
    "            values,\n",
    "            sigma,\n",
    "            attn_mask\n",
    "        )\n",
    "        out = out.view(B, L, -1)\n",
    "\n",
    "        return self.out_projection(out), series, prior, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TriangularCausalMask():\n",
    "    def __init__(self, B, L, device=\"cpu\"):\n",
    "        mask_shape = [B, 1, L, L]\n",
    "        with torch.no_grad():\n",
    "            self._mask = torch.triu(torch.ones(mask_shape, dtype=torch.bool), diagonal=1).to(device)\n",
    "\n",
    "    @property\n",
    "    def mask(self):\n",
    "        return self._mask\n",
    "\n",
    "\n",
    "class AnomalyAttention(nn.Module):\n",
    "    def __init__(self, win_size, mask_flag=True, scale=None, attention_dropout=0.0, output_attention=False):\n",
    "        super(AnomalyAttention, self).__init__()\n",
    "        self.scale = scale\n",
    "        self.mask_flag = mask_flag\n",
    "        self.output_attention = output_attention\n",
    "        self.dropout = nn.Dropout(attention_dropout)\n",
    "        window_size = win_size\n",
    "        self.distances = torch.zeros((window_size, window_size)).cuda()\n",
    "        for i in range(window_size):\n",
    "            for j in range(window_size):\n",
    "                self.distances[i][j] = abs(i - j)\n",
    "\n",
    "    def forward(self, queries, keys, values, sigma, attn_mask):\n",
    "        B, L, H, E = queries.shape\n",
    "        _, S, _, D = values.shape\n",
    "        scale = self.scale or 1. / math.sqrt(E)\n",
    "\n",
    "        scores = torch.einsum(\"blhe,bshe->bhls\", queries, keys)\n",
    "        if self.mask_flag:\n",
    "            if attn_mask is None:\n",
    "                attn_mask = TriangularCausalMask(B, L, device=queries.device)\n",
    "            scores.masked_fill_(attn_mask.mask, -np.inf)\n",
    "        attn = scale * scores\n",
    "\n",
    "        sigma = sigma.transpose(1, 2)  # B L H ->  B H L\n",
    "        window_size = attn.shape[-1]\n",
    "        sigma = torch.sigmoid(sigma * 5) + 1e-5\n",
    "        sigma = torch.pow(3, sigma) - 1\n",
    "        sigma = sigma.unsqueeze(-1).repeat(1, 1, 1, window_size)  # B H L L\n",
    "\n",
    "        prior = self.distances.unsqueeze(0).unsqueeze(0).repeat(sigma.shape[0], sigma.shape[1], 1, 1).cuda()\n",
    "        prior = 1.0 / (math.sqrt(2 * math.pi) * sigma) * torch.exp(-prior ** 2 / 2 / (sigma ** 2))\n",
    "\n",
    "        series = self.dropout(torch.softmax(attn, dim=-1))\n",
    "        V = torch.einsum(\"bhls,bshd->blhd\", series, values)\n",
    "\n",
    "        if self.output_attention:\n",
    "            return (V.contiguous(), series, prior, sigma)\n",
    "        else:\n",
    "            return (V.contiguous(), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, attention, d_model, d_ff=None, dropout=0.1, activation=\"relu\"):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        d_ff = d_ff or 4 * d_model\n",
    "        self.attention = attention\n",
    "        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = F.relu if activation == \"relu\" else F.gelu\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        new_x, attn, mask, sigma = self.attention(\n",
    "            x, x, x,\n",
    "            attn_mask=attn_mask\n",
    "        )\n",
    "        x = x + self.dropout(new_x)\n",
    "        y = x = self.norm1(x)\n",
    "        y = self.dropout(self.activation(self.conv1(y.transpose(-1, 1))))\n",
    "        y = self.dropout(self.conv2(y).transpose(-1, 1))\n",
    "\n",
    "        return self.norm2(x + y), attn, mask, sigma\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, attn_layers, norm_layer=None):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.attn_layers = nn.ModuleList(attn_layers)\n",
    "        self.norm = norm_layer\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        # x [B, L, D]\n",
    "        series_list = []\n",
    "        prior_list = []\n",
    "        sigma_list = []\n",
    "        for attn_layer in self.attn_layers:\n",
    "            x, series, prior, sigma = attn_layer(x, attn_mask=attn_mask)\n",
    "            series_list.append(series)\n",
    "            prior_list.append(prior)\n",
    "            sigma_list.append(sigma)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "\n",
    "        return x, series_list, prior_list, sigma_list\n",
    "\n",
    "\n",
    "class AnomalyTransformer(nn.Module):\n",
    "    def __init__(self, win_size, enc_in, c_out, d_model=512, n_heads=8, e_layers=2, d_ff=512,\n",
    "                 dropout=0.0, activation='gelu', output_attention=True):\n",
    "        super(AnomalyTransformer, self).__init__()\n",
    "        self.output_attention = output_attention\n",
    "\n",
    "        # Encoding\n",
    "        self.embedding = DataEmbedding(enc_in, d_model, cycle_length=80, dropout = dropout)\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = Encoder(\n",
    "            [\n",
    "                EncoderLayer(\n",
    "                    AttentionLayer(\n",
    "                        AnomalyAttention(win_size, False, attention_dropout=dropout, output_attention=output_attention),\n",
    "                        d_model, n_heads),\n",
    "                    d_model,\n",
    "                    d_ff,\n",
    "                    dropout=dropout,\n",
    "                    activation=activation\n",
    "                ) for l in range(e_layers)\n",
    "            ],\n",
    "            norm_layer=torch.nn.LayerNorm(d_model)\n",
    "        )\n",
    "\n",
    "        self.cls_head = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, d_model//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model//2, 2)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        enc_out = self.embedding(x)\n",
    "        enc_out, series, prior, sigmas = self.encoder(enc_out)\n",
    "        #feed the first cls token to the classifier\n",
    "        cls_out = self.cls_head(enc_out[:, 0, :])\n",
    "\n",
    "        if self.output_attention:\n",
    "            return cls_out, series, prior, sigmas\n",
    "        else:\n",
    "            return cls_out  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101, 1520, 3)\n"
     ]
    }
   ],
   "source": [
    "#load data\n",
    "train_data = pickle.load(open('packedData/train_transformer.pkl', 'rb'))\n",
    "test_data = pickle.load(open('packedData/test_transformer.pkl', 'rb'))\n",
    "\n",
    "print(np.array(train_data['eye']).shape)\n",
    "train_eye = torch.tensor(np.array(train_data['eye']), dtype=torch.float32)\n",
    "train_label = torch.tensor(np.array(train_data['label']), dtype=torch.long)\n",
    "\n",
    "test_eye = torch.tensor(np.array(test_data['eye']), dtype=torch.float32)\n",
    "test_label = torch.tensor(np.array(test_data['label']), dtype=torch.long)\n",
    "\n",
    "eye = torch.cat((train_eye, test_eye), 0)\n",
    "label = torch.cat((train_label, test_label), 0)\n",
    "\n",
    "\n",
    "#train\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "circle_length = 80\n",
    "sequence_length = 1520\n",
    "embedding_dim = 128\n",
    "num_heads = 4\n",
    "num_layers = 3\n",
    "num_classes = 2\n",
    "epochs = 1\n",
    "window_length = 20 #19 cycles + 1 cls token\n",
    "\n",
    "\n",
    "def my_kl_loss(p, q):\n",
    "\n",
    "    res = p * (torch.log(p + 1e-4) - torch.log(q + 1e-4))\n",
    "    return torch.mean(torch.sum(res, dim=-1), dim=1)\n",
    "\n",
    "\n",
    "def bootstrap_balance_minority(data, labels):\n",
    "\n",
    "    class_0_indices = (labels == 0).nonzero(as_tuple=True)[0]\n",
    "    class_1_indices = (labels == 1).nonzero(as_tuple=True)[0]\n",
    "\n",
    "    if len(class_0_indices) < len(class_1_indices):\n",
    "        minority_indices = class_0_indices\n",
    "        majority_indices = class_1_indices\n",
    "    else:\n",
    "        minority_indices = class_1_indices\n",
    "        majority_indices = class_0_indices\n",
    "    \n",
    "    num_minority_samples_needed = len(majority_indices) - len(minority_indices)\n",
    "    additional_samples = np.random.choice(minority_indices, size=num_minority_samples_needed, replace=True)\n",
    "    \n",
    "    balanced_minority_indices = torch.cat((minority_indices, torch.tensor(additional_samples)))\n",
    "    \n",
    "    balanced_indices = torch.cat((balanced_minority_indices, majority_indices))\n",
    "    \n",
    "    balanced_indices = balanced_indices[torch.randperm(len(balanced_indices))]\n",
    "\n",
    "    balanced_data = data[balanced_indices]\n",
    "    balanced_labels = labels[balanced_indices]\n",
    "\n",
    "    return balanced_data, balanced_labels\n",
    "\n",
    "\n",
    "def train(model, train_loader, val_loader, criterion, optimizer, device, k, alpha=0.8):\n",
    "    \"\"\"\n",
    "    Trains the model for one epoch using classification loss combined with association discrepancy.\n",
    "    k: weight factor for the discrepancy loss.\n",
    "    Returns:\n",
    "        train_accuracy, val_accuracy, val_loss\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    loss_list = []\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    import time\n",
    "    time_now = time.time()\n",
    "    iter_count = 0\n",
    "\n",
    "    for i, (input_data, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        iter_count += 1\n",
    "\n",
    "        # Move data to device\n",
    "        inputs = input_data.float().to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass: output is classification logits, and the model also returns attention info:\n",
    "        # series: list of series-association tensors from each encoder layer\n",
    "        # prior: list of prior-association tensors (computed via Gaussian kernel) from each encoder layer\n",
    "        # sigma: corresponding sigma values\n",
    "        output, series, prior, sigma = model(inputs)\n",
    "\n",
    "        # Classification loss on the CLS token output\n",
    "        cls_loss = criterion(output, labels)\n",
    "\n",
    "        # Compute association discrepancy losses over all layers\n",
    "        series_loss = 0.0\n",
    "        prior_loss = 0.0\n",
    "        # Assuming series and prior are lists (one per encoder layer)\n",
    "        for u in range(len(prior)):\n",
    "            prior_ex_cls = prior[u][:, :, 1:, 1:]\n",
    "            series_ex_cls = series[u][:, :, 1:, 1:]\n",
    "            \n",
    "            # Normalize the prior association along the last dimension\n",
    "            norm_prior = prior_ex_cls / torch.unsqueeze(torch.sum(prior_ex_cls, dim=-1), dim=-1)\n",
    "            \n",
    "            # Compute the KL divergence-based losses\n",
    "            series_loss += (torch.mean(my_kl_loss(series_ex_cls, norm_prior.detach())) +\n",
    "                            torch.mean(my_kl_loss(norm_prior.detach(), series_ex_cls)))\n",
    "            prior_loss += (torch.mean(my_kl_loss(norm_prior, series_ex_cls.detach())) +\n",
    "                        torch.mean(my_kl_loss(series_ex_cls.detach(), norm_prior)))\n",
    "        series_loss = series_loss / len(prior)\n",
    "        prior_loss = prior_loss / len(prior)\n",
    "\n",
    "\n",
    "        # Define two losses for minimax strategy:\n",
    "        # loss1 encourages larger discrepancy (by subtracting the series loss)\n",
    "        # loss2 encourages the opposite for the prior branch\n",
    "        loss1 = - k * series_loss \n",
    "        loss2 = cls_loss + k * prior_loss \n",
    "\n",
    "        # Backpropagate in two phases (minimax strategy)\n",
    "        loss1.backward(retain_graph=True)\n",
    "        loss2.backward()\n",
    "        #cls_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_list.append(cls_loss.item())\n",
    "\n",
    "        # Calculate training accuracy for this batch\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            speed = (time.time() - time_now) / iter_count\n",
    "            print(f'\\tIteration {i+1}: speed: {speed:.4f}s/iter')\n",
    "            iter_count = 0\n",
    "            time_now = time.time()\n",
    "\n",
    "    avg_train_loss = np.mean(loss_list)\n",
    "    train_accuracy = total_correct / total_samples * 100\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    val_loss, val_accuracy = validate(model, val_loader, criterion, device)\n",
    "    return train_accuracy, val_accuracy, val_loss\n",
    "\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_data, labels in val_loader:\n",
    "            inputs = input_data.float().to(device)\n",
    "            labels = labels.to(device)\n",
    "            output, series, prior, sigma = model(inputs)\n",
    "            loss = criterion(output, labels)\n",
    "            total_loss += loss.item() * labels.size(0)\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / total_samples\n",
    "    accuracy = total_correct / total_samples * 100\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 379502\n",
      "Fold 0\n",
      "Epoch 0: Train Acc = 46.91, Val Acc = 50.00, Val Loss = 0.6959\n",
      "Epoch 1: Train Acc = 49.38, Val Acc = 50.00, Val Loss = 0.6974\n",
      "Epoch 2: Train Acc = 53.70, Val Acc = 50.00, Val Loss = 0.6887\n",
      "Epoch 3: Train Acc = 55.56, Val Acc = 50.00, Val Loss = 0.6865\n",
      "Epoch 4: Train Acc = 50.62, Val Acc = 50.00, Val Loss = 0.6856\n",
      "Epoch 5: Train Acc = 56.79, Val Acc = 50.00, Val Loss = 0.6828\n",
      "Epoch 6: Train Acc = 51.85, Val Acc = 50.00, Val Loss = 0.6791\n",
      "Epoch 7: Train Acc = 62.35, Val Acc = 50.00, Val Loss = 0.6754\n",
      "Epoch 8: Train Acc = 58.64, Val Acc = 80.95, Val Loss = 0.6681\n",
      "Epoch 9: Train Acc = 53.70, Val Acc = 61.90, Val Loss = 0.6600\n",
      "Epoch 10: Train Acc = 58.02, Val Acc = 88.10, Val Loss = 0.6475\n",
      "Early stopping triggered\n",
      "Epoch 11: Train Acc = 56.79, Val Acc = 90.48, Val Loss = 0.6337\n",
      "Early stopping triggered\n",
      "Epoch 12: Train Acc = 61.11, Val Acc = 85.71, Val Loss = 0.6083\n",
      "Epoch 13: Train Acc = 69.75, Val Acc = 83.33, Val Loss = 0.5743\n",
      "Epoch 14: Train Acc = 65.43, Val Acc = 83.33, Val Loss = 0.5223\n",
      "Epoch 15: Train Acc = 74.69, Val Acc = 85.71, Val Loss = 0.4642\n",
      "Epoch 16: Train Acc = 75.93, Val Acc = 85.71, Val Loss = 0.4023\n",
      "Epoch 17: Train Acc = 79.01, Val Acc = 85.71, Val Loss = 0.3921\n",
      "Epoch 18: Train Acc = 79.01, Val Acc = 88.10, Val Loss = 0.3070\n",
      "Early stopping triggered\n",
      "Epoch 19: Train Acc = 85.19, Val Acc = 90.48, Val Loss = 0.2832\n",
      "Early stopping triggered\n",
      "Epoch 20: Train Acc = 84.57, Val Acc = 85.71, Val Loss = 0.3449\n",
      "Epoch 21: Train Acc = 90.74, Val Acc = 90.48, Val Loss = 0.2736\n",
      "Early stopping triggered\n",
      "Epoch 22: Train Acc = 91.36, Val Acc = 88.10, Val Loss = 0.2704\n",
      "Early stopping triggered\n",
      "Epoch 23: Train Acc = 93.21, Val Acc = 88.10, Val Loss = 0.3031\n",
      "Early stopping triggered\n",
      "Epoch 24: Train Acc = 93.83, Val Acc = 83.33, Val Loss = 0.2774\n",
      "Epoch 25: Train Acc = 94.44, Val Acc = 83.33, Val Loss = 0.2880\n",
      "Epoch 26: Train Acc = 95.06, Val Acc = 83.33, Val Loss = 0.3230\n",
      "Epoch 27: Train Acc = 94.44, Val Acc = 83.33, Val Loss = 0.3626\n",
      "Epoch    29: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch 28: Train Acc = 95.68, Val Acc = 85.71, Val Loss = 0.5080\n",
      "Epoch 29: Train Acc = 95.06, Val Acc = 85.71, Val Loss = 0.3461\n",
      "Epoch 30: Train Acc = 95.06, Val Acc = 83.33, Val Loss = 0.3938\n",
      "Epoch 31: Train Acc = 95.68, Val Acc = 83.33, Val Loss = 0.3955\n",
      "Epoch 32: Train Acc = 95.06, Val Acc = 83.33, Val Loss = 0.4059\n",
      "Epoch 33: Train Acc = 96.91, Val Acc = 83.33, Val Loss = 0.3964\n",
      "Epoch 34: Train Acc = 96.91, Val Acc = 83.33, Val Loss = 0.3918\n",
      "Epoch    36: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch 35: Train Acc = 96.91, Val Acc = 83.33, Val Loss = 0.4371\n",
      "Epoch 36: Train Acc = 97.53, Val Acc = 83.33, Val Loss = 0.4387\n",
      "Epoch 37: Train Acc = 96.30, Val Acc = 83.33, Val Loss = 0.4417\n",
      "Epoch 38: Train Acc = 96.91, Val Acc = 83.33, Val Loss = 0.4473\n",
      "Epoch 39: Train Acc = 97.53, Val Acc = 83.33, Val Loss = 0.4515\n",
      "Number of parameters: 379502\n",
      "Fold 1\n",
      "Epoch 0: Train Acc = 45.06, Val Acc = 50.00, Val Loss = 0.6905\n",
      "Epoch 1: Train Acc = 50.62, Val Acc = 50.00, Val Loss = 0.6901\n",
      "Epoch 2: Train Acc = 51.85, Val Acc = 50.00, Val Loss = 0.6879\n",
      "Epoch 3: Train Acc = 53.09, Val Acc = 50.00, Val Loss = 0.6869\n",
      "Epoch 4: Train Acc = 47.53, Val Acc = 50.00, Val Loss = 0.6908\n",
      "Epoch 5: Train Acc = 53.09, Val Acc = 76.19, Val Loss = 0.6838\n",
      "Epoch 6: Train Acc = 52.47, Val Acc = 85.71, Val Loss = 0.6812\n",
      "Epoch 7: Train Acc = 56.17, Val Acc = 85.71, Val Loss = 0.6766\n",
      "Epoch 8: Train Acc = 54.32, Val Acc = 85.71, Val Loss = 0.6690\n",
      "Epoch 9: Train Acc = 61.11, Val Acc = 57.14, Val Loss = 0.6633\n",
      "Epoch 10: Train Acc = 62.96, Val Acc = 88.10, Val Loss = 0.6519\n",
      "Early stopping triggered\n",
      "Epoch 11: Train Acc = 63.58, Val Acc = 83.33, Val Loss = 0.6380\n",
      "Epoch 12: Train Acc = 62.96, Val Acc = 80.95, Val Loss = 0.6194\n",
      "Epoch 13: Train Acc = 72.84, Val Acc = 80.95, Val Loss = 0.5854\n",
      "Epoch 14: Train Acc = 71.60, Val Acc = 88.10, Val Loss = 0.5415\n",
      "Early stopping triggered\n",
      "Epoch 15: Train Acc = 77.16, Val Acc = 78.57, Val Loss = 0.4968\n",
      "Epoch 16: Train Acc = 80.25, Val Acc = 85.71, Val Loss = 0.4131\n",
      "Epoch 17: Train Acc = 80.86, Val Acc = 85.71, Val Loss = 0.3755\n",
      "Epoch 18: Train Acc = 85.19, Val Acc = 78.57, Val Loss = 0.4498\n",
      "Epoch 19: Train Acc = 88.27, Val Acc = 90.48, Val Loss = 0.3427\n",
      "Early stopping triggered\n",
      "Epoch 20: Train Acc = 91.98, Val Acc = 76.19, Val Loss = 0.4716\n",
      "Epoch 21: Train Acc = 90.12, Val Acc = 80.95, Val Loss = 0.3933\n",
      "Epoch 22: Train Acc = 91.98, Val Acc = 80.95, Val Loss = 0.3861\n",
      "Epoch 23: Train Acc = 92.59, Val Acc = 80.95, Val Loss = 0.4289\n",
      "Epoch 24: Train Acc = 88.89, Val Acc = 88.10, Val Loss = 0.3318\n",
      "Early stopping triggered\n",
      "Epoch 25: Train Acc = 90.74, Val Acc = 85.71, Val Loss = 0.3646\n",
      "Epoch 26: Train Acc = 93.21, Val Acc = 88.10, Val Loss = 0.3479\n",
      "Early stopping triggered\n",
      "Epoch 27: Train Acc = 93.83, Val Acc = 85.71, Val Loss = 0.4559\n",
      "Epoch 28: Train Acc = 95.68, Val Acc = 76.19, Val Loss = 0.5678\n",
      "Epoch 29: Train Acc = 94.44, Val Acc = 88.10, Val Loss = 0.3719\n",
      "Early stopping triggered\n",
      "Epoch    31: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch 30: Train Acc = 95.68, Val Acc = 85.71, Val Loss = 0.4718\n",
      "Epoch 31: Train Acc = 93.83, Val Acc = 88.10, Val Loss = 0.3495\n",
      "Early stopping triggered\n",
      "Epoch 32: Train Acc = 96.91, Val Acc = 90.48, Val Loss = 0.3347\n",
      "Early stopping triggered\n",
      "Epoch 33: Train Acc = 97.53, Val Acc = 88.10, Val Loss = 0.3916\n",
      "Early stopping triggered\n",
      "Epoch 34: Train Acc = 96.91, Val Acc = 88.10, Val Loss = 0.3529\n",
      "Early stopping triggered\n",
      "Epoch 35: Train Acc = 97.53, Val Acc = 88.10, Val Loss = 0.3881\n",
      "Early stopping triggered\n",
      "Epoch 36: Train Acc = 96.91, Val Acc = 88.10, Val Loss = 0.3688\n",
      "Early stopping triggered\n",
      "Epoch    38: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch 37: Train Acc = 97.53, Val Acc = 88.10, Val Loss = 0.3910\n",
      "Early stopping triggered\n",
      "Epoch 38: Train Acc = 97.53, Val Acc = 88.10, Val Loss = 0.3983\n",
      "Early stopping triggered\n",
      "Epoch 39: Train Acc = 97.53, Val Acc = 88.10, Val Loss = 0.3965\n",
      "Early stopping triggered\n",
      "Number of parameters: 379502\n",
      "Fold 2\n",
      "Epoch 0: Train Acc = 48.78, Val Acc = 50.00, Val Loss = 0.6878\n",
      "Epoch 1: Train Acc = 50.00, Val Acc = 50.00, Val Loss = 0.6855\n",
      "Epoch 2: Train Acc = 51.83, Val Acc = 70.00, Val Loss = 0.6788\n",
      "Epoch 3: Train Acc = 53.05, Val Acc = 60.00, Val Loss = 0.6747\n",
      "Epoch 4: Train Acc = 59.15, Val Acc = 60.00, Val Loss = 0.6695\n",
      "Epoch 5: Train Acc = 57.93, Val Acc = 77.50, Val Loss = 0.6544\n",
      "Epoch 6: Train Acc = 64.02, Val Acc = 60.00, Val Loss = 0.6530\n",
      "Epoch 7: Train Acc = 75.00, Val Acc = 55.00, Val Loss = 0.6275\n",
      "Epoch 8: Train Acc = 74.39, Val Acc = 65.00, Val Loss = 0.5994\n",
      "Epoch 9: Train Acc = 77.44, Val Acc = 65.00, Val Loss = 0.5938\n",
      "Epoch 10: Train Acc = 80.49, Val Acc = 67.50, Val Loss = 0.6295\n",
      "Epoch 11: Train Acc = 84.76, Val Acc = 65.00, Val Loss = 0.5786\n",
      "Epoch 12: Train Acc = 81.71, Val Acc = 70.00, Val Loss = 0.8287\n",
      "Epoch 13: Train Acc = 87.80, Val Acc = 70.00, Val Loss = 0.8655\n",
      "Epoch 14: Train Acc = 89.02, Val Acc = 70.00, Val Loss = 0.9560\n",
      "Epoch 15: Train Acc = 92.07, Val Acc = 70.00, Val Loss = 0.9118\n",
      "Epoch 16: Train Acc = 90.85, Val Acc = 67.50, Val Loss = 0.7495\n",
      "Epoch    18: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch 17: Train Acc = 92.68, Val Acc = 67.50, Val Loss = 0.7563\n",
      "Epoch 18: Train Acc = 94.51, Val Acc = 70.00, Val Loss = 1.1271\n",
      "Epoch 19: Train Acc = 93.29, Val Acc = 70.00, Val Loss = 1.0135\n",
      "Epoch 20: Train Acc = 93.29, Val Acc = 70.00, Val Loss = 1.0594\n",
      "Epoch 21: Train Acc = 96.34, Val Acc = 70.00, Val Loss = 1.0698\n",
      "Epoch 22: Train Acc = 94.51, Val Acc = 70.00, Val Loss = 1.2314\n",
      "Epoch 23: Train Acc = 95.12, Val Acc = 70.00, Val Loss = 1.1968\n",
      "Epoch    25: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch 24: Train Acc = 96.34, Val Acc = 70.00, Val Loss = 1.2891\n",
      "Epoch 25: Train Acc = 97.56, Val Acc = 70.00, Val Loss = 1.1768\n",
      "Epoch 26: Train Acc = 96.95, Val Acc = 70.00, Val Loss = 1.2493\n",
      "Epoch 27: Train Acc = 96.95, Val Acc = 70.00, Val Loss = 1.2674\n",
      "Epoch 28: Train Acc = 97.56, Val Acc = 70.00, Val Loss = 1.3019\n",
      "Epoch 29: Train Acc = 97.56, Val Acc = 70.00, Val Loss = 1.3438\n",
      "Epoch 30: Train Acc = 96.95, Val Acc = 70.00, Val Loss = 1.3238\n",
      "Epoch    32: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch 31: Train Acc = 98.78, Val Acc = 70.00, Val Loss = 1.3176\n",
      "Epoch 32: Train Acc = 98.78, Val Acc = 70.00, Val Loss = 1.3021\n",
      "Epoch 33: Train Acc = 98.17, Val Acc = 70.00, Val Loss = 1.3157\n",
      "Epoch 34: Train Acc = 98.78, Val Acc = 70.00, Val Loss = 1.3075\n",
      "Epoch 35: Train Acc = 96.95, Val Acc = 70.00, Val Loss = 1.3097\n",
      "Epoch 36: Train Acc = 96.95, Val Acc = 70.00, Val Loss = 1.3306\n",
      "Epoch 37: Train Acc = 98.17, Val Acc = 70.00, Val Loss = 1.2714\n",
      "Epoch    39: reducing learning rate of group 0 to 6.2500e-06.\n",
      "Epoch 38: Train Acc = 98.78, Val Acc = 70.00, Val Loss = 1.2930\n",
      "Epoch 39: Train Acc = 98.78, Val Acc = 70.00, Val Loss = 1.3429\n",
      "Number of parameters: 379502\n",
      "Fold 3\n",
      "Epoch 0: Train Acc = 46.34, Val Acc = 50.00, Val Loss = 0.6932\n",
      "Epoch 1: Train Acc = 50.00, Val Acc = 50.00, Val Loss = 0.6894\n",
      "Epoch 2: Train Acc = 46.95, Val Acc = 50.00, Val Loss = 0.6886\n",
      "Epoch 3: Train Acc = 50.00, Val Acc = 50.00, Val Loss = 0.6875\n",
      "Epoch 4: Train Acc = 50.00, Val Acc = 50.00, Val Loss = 0.6850\n",
      "Epoch 5: Train Acc = 51.83, Val Acc = 62.50, Val Loss = 0.6810\n",
      "Epoch 6: Train Acc = 54.88, Val Acc = 70.00, Val Loss = 0.6791\n",
      "Epoch 7: Train Acc = 60.98, Val Acc = 62.50, Val Loss = 0.6775\n",
      "Epoch 8: Train Acc = 52.44, Val Acc = 80.00, Val Loss = 0.6732\n",
      "Epoch 9: Train Acc = 57.32, Val Acc = 62.50, Val Loss = 0.6663\n",
      "Epoch 10: Train Acc = 57.32, Val Acc = 72.50, Val Loss = 0.6565\n",
      "Epoch 11: Train Acc = 67.07, Val Acc = 87.50, Val Loss = 0.6423\n",
      "Epoch 12: Train Acc = 66.46, Val Acc = 62.50, Val Loss = 0.6239\n",
      "Epoch 13: Train Acc = 69.51, Val Acc = 72.50, Val Loss = 0.5929\n",
      "Epoch 14: Train Acc = 78.66, Val Acc = 82.50, Val Loss = 0.5571\n",
      "Epoch 15: Train Acc = 79.88, Val Acc = 70.00, Val Loss = 0.5035\n",
      "Epoch 16: Train Acc = 75.61, Val Acc = 87.50, Val Loss = 0.4514\n",
      "Epoch 17: Train Acc = 81.71, Val Acc = 72.50, Val Loss = 0.4199\n",
      "Epoch 18: Train Acc = 84.15, Val Acc = 82.50, Val Loss = 0.4337\n",
      "Epoch 19: Train Acc = 89.02, Val Acc = 90.00, Val Loss = 0.3658\n",
      "Early stopping triggered\n",
      "Epoch 20: Train Acc = 86.59, Val Acc = 90.00, Val Loss = 0.3711\n",
      "Early stopping triggered\n",
      "Epoch 21: Train Acc = 90.24, Val Acc = 90.00, Val Loss = 0.3658\n",
      "Early stopping triggered\n",
      "Epoch 22: Train Acc = 92.07, Val Acc = 90.00, Val Loss = 0.3669\n",
      "Early stopping triggered\n",
      "Epoch 23: Train Acc = 91.46, Val Acc = 90.00, Val Loss = 0.3829\n",
      "Early stopping triggered\n",
      "Epoch 24: Train Acc = 93.90, Val Acc = 90.00, Val Loss = 0.4247\n",
      "Early stopping triggered\n",
      "Epoch    26: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch 25: Train Acc = 95.12, Val Acc = 77.50, Val Loss = 0.5120\n",
      "Epoch 26: Train Acc = 96.34, Val Acc = 72.50, Val Loss = 0.5205\n",
      "Epoch 27: Train Acc = 96.95, Val Acc = 72.50, Val Loss = 0.5556\n",
      "Epoch 28: Train Acc = 98.17, Val Acc = 75.00, Val Loss = 0.5688\n",
      "Epoch 29: Train Acc = 97.56, Val Acc = 55.00, Val Loss = 0.6742\n",
      "Epoch 30: Train Acc = 97.56, Val Acc = 55.00, Val Loss = 1.0736\n",
      "Epoch 31: Train Acc = 95.73, Val Acc = 62.50, Val Loss = 0.5795\n",
      "Epoch    33: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch 32: Train Acc = 98.78, Val Acc = 92.50, Val Loss = 0.4925\n",
      "Early stopping triggered\n",
      "Epoch 33: Train Acc = 98.78, Val Acc = 75.00, Val Loss = 0.5113\n",
      "Epoch 34: Train Acc = 96.34, Val Acc = 92.50, Val Loss = 0.5286\n",
      "Early stopping triggered\n",
      "Epoch 35: Train Acc = 97.56, Val Acc = 62.50, Val Loss = 0.5982\n",
      "Epoch 36: Train Acc = 97.56, Val Acc = 55.00, Val Loss = 0.7591\n",
      "Epoch 37: Train Acc = 97.56, Val Acc = 92.50, Val Loss = 0.5243\n",
      "Early stopping triggered\n",
      "Epoch 38: Train Acc = 98.78, Val Acc = 62.50, Val Loss = 0.6263\n",
      "Epoch    40: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch 39: Train Acc = 98.78, Val Acc = 92.50, Val Loss = 0.5311\n",
      "Early stopping triggered\n",
      "Number of parameters: 379502\n",
      "Fold 4\n",
      "Epoch 0: Train Acc = 53.05, Val Acc = 50.00, Val Loss = 0.6891\n",
      "Epoch 1: Train Acc = 53.66, Val Acc = 65.00, Val Loss = 0.6866\n",
      "Epoch 2: Train Acc = 54.27, Val Acc = 57.50, Val Loss = 0.6847\n",
      "Epoch 3: Train Acc = 50.00, Val Acc = 50.00, Val Loss = 0.6831\n",
      "Epoch 4: Train Acc = 57.32, Val Acc = 50.00, Val Loss = 0.6807\n",
      "Epoch 5: Train Acc = 60.98, Val Acc = 60.00, Val Loss = 0.6744\n",
      "Epoch 6: Train Acc = 60.37, Val Acc = 70.00, Val Loss = 0.6645\n",
      "Epoch 7: Train Acc = 64.63, Val Acc = 77.50, Val Loss = 0.6533\n",
      "Epoch 8: Train Acc = 60.37, Val Acc = 67.50, Val Loss = 0.6369\n",
      "Epoch 9: Train Acc = 69.51, Val Acc = 62.50, Val Loss = 0.6168\n",
      "Epoch 10: Train Acc = 72.56, Val Acc = 70.00, Val Loss = 0.5839\n",
      "Epoch 11: Train Acc = 77.44, Val Acc = 75.00, Val Loss = 0.5592\n",
      "Epoch 12: Train Acc = 81.71, Val Acc = 70.00, Val Loss = 0.5881\n",
      "Epoch 13: Train Acc = 79.27, Val Acc = 80.00, Val Loss = 0.5088\n",
      "Epoch 14: Train Acc = 80.49, Val Acc = 67.50, Val Loss = 0.4839\n",
      "Epoch 15: Train Acc = 84.15, Val Acc = 75.00, Val Loss = 0.5203\n",
      "Epoch 16: Train Acc = 84.76, Val Acc = 90.00, Val Loss = 0.3861\n",
      "Early stopping triggered\n",
      "Epoch 17: Train Acc = 86.59, Val Acc = 92.50, Val Loss = 0.3591\n",
      "Early stopping triggered\n",
      "Epoch 18: Train Acc = 85.37, Val Acc = 92.50, Val Loss = 0.3608\n",
      "Early stopping triggered\n",
      "Epoch 19: Train Acc = 89.63, Val Acc = 92.50, Val Loss = 0.3405\n",
      "Early stopping triggered\n",
      "Epoch 20: Train Acc = 89.02, Val Acc = 90.00, Val Loss = 0.3463\n",
      "Early stopping triggered\n",
      "Epoch 21: Train Acc = 92.68, Val Acc = 92.50, Val Loss = 0.3201\n",
      "Early stopping triggered\n",
      "Epoch 22: Train Acc = 89.63, Val Acc = 90.00, Val Loss = 0.3151\n",
      "Early stopping triggered\n",
      "Epoch 23: Train Acc = 93.29, Val Acc = 87.50, Val Loss = 0.3712\n",
      "Epoch 24: Train Acc = 92.07, Val Acc = 90.00, Val Loss = 0.3107\n",
      "Early stopping triggered\n",
      "Epoch 25: Train Acc = 94.51, Val Acc = 80.00, Val Loss = 0.3297\n",
      "Epoch 26: Train Acc = 92.68, Val Acc = 80.00, Val Loss = 0.3416\n",
      "Epoch 27: Train Acc = 94.51, Val Acc = 77.50, Val Loss = 0.3428\n",
      "Epoch 28: Train Acc = 90.24, Val Acc = 90.00, Val Loss = 0.3432\n",
      "Early stopping triggered\n",
      "Epoch 29: Train Acc = 91.46, Val Acc = 87.50, Val Loss = 0.4066\n",
      "Epoch    31: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch 30: Train Acc = 94.51, Val Acc = 90.00, Val Loss = 0.3494\n",
      "Early stopping triggered\n",
      "Epoch 31: Train Acc = 96.34, Val Acc = 90.00, Val Loss = 0.3441\n",
      "Early stopping triggered\n",
      "Epoch 32: Train Acc = 95.73, Val Acc = 90.00, Val Loss = 0.3404\n",
      "Early stopping triggered\n",
      "Epoch 33: Train Acc = 96.95, Val Acc = 90.00, Val Loss = 0.3521\n",
      "Early stopping triggered\n",
      "Epoch 34: Train Acc = 97.56, Val Acc = 90.00, Val Loss = 0.3527\n",
      "Early stopping triggered\n",
      "Epoch 35: Train Acc = 95.73, Val Acc = 77.50, Val Loss = 0.3628\n",
      "Epoch 36: Train Acc = 97.56, Val Acc = 90.00, Val Loss = 0.3817\n",
      "Early stopping triggered\n",
      "Epoch    38: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch 37: Train Acc = 96.34, Val Acc = 90.00, Val Loss = 0.4197\n",
      "Early stopping triggered\n",
      "Epoch 38: Train Acc = 94.51, Val Acc = 90.00, Val Loss = 0.3517\n",
      "Early stopping triggered\n",
      "Epoch 39: Train Acc = 96.95, Val Acc = 90.00, Val Loss = 0.3601\n",
      "Early stopping triggered\n",
      "Average Validation Accuracy: 88.69047619047619\n",
      "Fold Validation Accuracies: [90.47619047619048, 90.47619047619048, 77.5, 92.5, 92.5]\n",
      "Average Training Accuracy: 97.91478470340259\n",
      "Fold Training Accuracies: [97.53086419753086, 97.53086419753086, 98.78048780487805, 98.78048780487805, 96.95121951219512]\n"
     ]
    }
   ],
   "source": [
    "epochs = 40\n",
    "k_fold = 5\n",
    "kf = StratifiedKFold(n_splits=k_fold, shuffle=True, random_state=42)\n",
    "fold_accuracy = []\n",
    "fold_train_accuracy = []\n",
    "\n",
    "early_stop_patience = 4   # number of epochs with no improvement before stopping\n",
    "min_delta = 1e-4          # minimum change to qualify as an improvement\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(kf.split(eye, label)):\n",
    "    # Define the classification loss (cross-entropy)\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    \n",
    "    # Instantiate the revised anomaly transformer model.\n",
    "    model = AnomalyTransformer(\n",
    "        win_size=window_length,  # total tokens: 20 (19 cycles + 1 CLS token)\n",
    "        enc_in=3,                # input channels from eye data\n",
    "        c_out=num_classes,\n",
    "        d_model=embedding_dim,\n",
    "        n_heads=num_heads,\n",
    "        e_layers=num_layers,\n",
    "        d_ff=embedding_dim,       # adjust if needed\n",
    "        dropout=0.3,\n",
    "        activation='gelu',\n",
    "        output_attention=True\n",
    "    ).to(device)\n",
    "\n",
    "    #print number of parameters\n",
    "    print(f\"Number of parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=2e-2)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, factor=0.5, patience=5, cooldown=0.5, min_lr=5e-6, verbose=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Fold {fold}\")\n",
    "    # Split the dataset using the provided indices\n",
    "    train_eye_fold, test_eye_fold = eye[train_index], eye[test_index]\n",
    "    train_label_fold, test_label_fold = label[train_index], label[test_index]\n",
    "\n",
    "    # Bootstrap balance the minority class for training and validation sets\n",
    "    train_eye_fold, train_label_fold = bootstrap_balance_minority(train_eye_fold, train_label_fold)\n",
    "    test_eye_fold, test_label_fold = bootstrap_balance_minority(test_eye_fold, test_label_fold)\n",
    "\n",
    "    train_dataset = torch.utils.data.TensorDataset(train_eye_fold, train_label_fold)\n",
    "    val_dataset = torch.utils.data.TensorDataset(test_eye_fold, test_label_fold)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    best_val_acc = 0\n",
    "    previous_val_acc = 0\n",
    "    patience_counter = 0\n",
    "    for epoch in range(epochs):\n",
    "        # Use the modified training function that combines classification loss with discrepancy loss.\n",
    "        train_accuracy, val_accuracy, val_loss = train(\n",
    "            model, train_loader, val_loader, criterion, optimizer, device, k=0, alpha=0\n",
    "        )\n",
    "        scheduler.step(val_loss)\n",
    "        print(f\"Epoch {epoch}: Train Acc = {train_accuracy:.2f}, Val Acc = {val_accuracy:.2f}, Val Loss = {val_loss:.4f}\")\n",
    "        \n",
    "        # Check if validation loss improved by at least min_delta\n",
    "        if  val_accuracy - previous_val_acc >=0:\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if val_accuracy > best_val_acc:\n",
    "            best_val_acc = val_accuracy\n",
    "        \n",
    "        if patience_counter >= early_stop_patience or val_accuracy >= 88.0:\n",
    "            print(\"Early stopping triggered\")\n",
    "            #break\n",
    "        previous_val_acc = val_accuracy\n",
    "\n",
    "    fold_accuracy.append(best_val_acc)\n",
    "    fold_train_accuracy.append(train_accuracy)\n",
    "\n",
    "print(f\"Average Validation Accuracy: {np.mean(fold_accuracy)}\")\n",
    "print(\"Fold Validation Accuracies:\", fold_accuracy)\n",
    "print(f\"Average Training Accuracy: {np.mean(fold_train_accuracy)}\")\n",
    "print(\"Fold Training Accuracies:\", fold_train_accuracy)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
